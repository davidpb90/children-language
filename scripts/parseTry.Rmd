---
title: "Project Year 1"
author: David Pardo
date: April 3, 2015
output: pdf_document
---

Preparing IS dataset
========================================================

## Preliminaries:

Setting wroking directory where the dataset is located and loading libraries for text mining and string editing

```{r results="hide", warning=FALSE, message=FALSE}
setwd("~/Desktop/Complexity1/Project/sector")

library(tm)
library(stringi)
library(proxy)
library(XML)
library(tm)
library(MASS)
library(ggplot2)
library(fitdistrplus)
library(poweRlaw)
library(stringi)
library(compiler)
library(stringr)
library(SnowballC)
library(RWeka)
library(rJava)
library(RWekajars)
```

## Creating the corpus

Preparing the list of url's:

```{r eval=FALSE, warning=FALSE}
#Creates list of all url's in the dataset 
html1<-list.files(recursive=TRUE,pattern="\\.(htm|html)$")
```

A function that creates a corpus for each html document:

```{r eval=FALSE, warning=FALSE}
#Function to create a corpus with the document in url.name
create.corpus <- function(url.name){
  text<-""
  conn<-file(url.name,open="r")
  linn<-readLines(conn)
  for (j in 10:length(linn)){
    text<-stri_flatten(c(text,linn[j]),col = " ")
  }
  close(conn)
  cc=Corpus(VectorSource(text))
  meta(cc,"link")=url.name
  return(cc)
}
```

Creates a single corpus with all the documents in the dataset:

```{r eval=FALSE, warning=FALSE}
#Create a corpora where each document is a corpus
cc <- lapply(html1, create.corpus)
#Create only one corpus out of all the documents
docs = Corpus(VectorSource(unlist(cc)))
meta(docs,'link') = do.call(rbind,lapply(cc,meta))$link
```

## Parsing the corpus

The documents are parsed and transformed into plain text while removing, numbers, punctuation and certain words:

```{r eval=FALSE, warning=FALSE}
#Removes html language
docs <- tm_map(docs, function(x) stri_replace_all_regex(x, "<.+?>", " "))
#Removes all \t
docs <- tm_map(docs, function(x) stri_replace_all_fixed(x, "\t", " "))
#Transforms the document into plain text format
docs <- tm_map(docs, PlainTextDocument)
#Removes english stopwords and "nbsp"
docs <- tm_map(docs, removeWords, c(stopwords("english"),"nbsp"))
#Removes punctuation
docs <- tm_map(docs, removePunctuation)
#Trasnforms all capital letters into lower case
docs <- tm_map(docs, tolower)
#Removes numbers
docs <- tm_map(docs, removeNumbers)
#Function to remove all url's
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
docs <- tm_map(docs, removeWords, c(stopwords("english"),"th","s"))
#Removes url's
docs <- tm_map(docs, removeURL)
#Removes extra whitespaces
docs <- tm_map(docs, stripWhitespace)
```

## Stemming the corpus

The words are stemmed to count as 1 word words with the same root.

Stem completion was too slow and is not useful for the current statistical analysis.

```{r eval=FALSE, warning=FALSE}
# make sure that packages below have been installed
# Otherwise, stem completion will fail.
#Libraries for stemming
library(SnowballC)
library(RWeka)
library(rJava)
library(RWekajars)
# keep a copy of corpus to use later as a dictionary for stem completion
 docsCopy <- docs
# stem words
docs <- tm_map(docs, stemDocument,language="english")
```

The final corpus is stored and can be retrieved any time from memory, instead of doing the whole process over and over.

First the working directory is set and the lines are written into a .txt file.
```{r}
setwd("~/Desktop/Complexity1/Project")
```

```{r eval=FALSE, warning=FALSE}
writeLines(as.character(docs), con="stemCorpus.txt")
```

This text file can be read to obtain the corpus anew.
```{r}
conn<-file("stemCorpus.txt",open="r")
d<-readLines(conn)
close(conn)
docs<-Corpus(VectorSource(d))
```

## Creating an artificial corpus resembling the statistical features of the obtained corpus

Some libraries that are required:
```{r results="hide", warning=FALSE, message=FALSE}
#Text mining
library(tm)
#Histograms
library(MASS)
#Nice plots
library(ggplot2)
#Fits power law distributions
library(fitdistrplus)
library(poweRlaw)
#Manipulates strings
library(stringi)
#Compiles code
library(compiler)
```

We create a Term Document Matrix, each row corresponds to a word and each column to a document; position [i,j] is 1 if word i is in document j and is 0 otherwise.

```{r, warning=FALSE}
Tdm <- TermDocumentMatrix(docs, control=list(wordLengths=c(1,Inf)))
```

# Lognormal fitting of the document lengths 

We want to fit the length of the documents to a lognormal ditribution finding the best parameter through MLE. First we get the total number of documents and terms of the original corpus. We proceed to get a vector with the number of words in each document. 

```{r, warning=FALSE}
x<-t(Tdm)
#Number of documents
numDocs<-nrow(x)
#Number of terms
numTerms<-ncol(x)
#Number of words in each document
nOW<-as.vector(slam::row_sums(x))
```

The MLE is performed. 

```{r, warning=FALSE}
#Maximum likelihood estimation for the lognormal
mle<-fitdistr(nOW[nOW>8], "lognormal")
meanlogemp <- mle$estimate["meanlog"]
sdlogemp <- mle$estimate["sdlog"]

```

A goodness-of-fit test is done over this distribution. Through trial and error it was found that the best fitting required the documents with less than 8 words to be omitted.

```{r, warning=FALSE}
#Kolmogorov-Smirnov test.
ks.test(nOW[nOW>8], plnorm, meanlog = meanlogemp, sdlog = sdlogemp) 
```

We plot the data and the fitted distribution.

```{r, warning=FALSE}
#Number of words in the biggest document
maxDoc<-max(nOW)
mydata_hist<-hist(nOW,probability = TRUE, breaks = maxDoc, col = "darkslategray4", border = "seashell3")
z<-seq(0,1500,1)

#Calculates the probability that a document has certain number of words, using data from the histogram produced above.
prob<-mydata_hist$count/numDocs
#Plot empirical vs fitted distribution density functions
plot(mydata_hist$mid,log="xy",prob,lwd=1, lend=1)
lines(dlnorm(z,meanlog = meanlogemp, sdlog = sdlogemp),   # Add the kernel density estimate (-.5 fix for the bins)
      col = "firebrick2", lwd = 3)
```

# Creating a new corpus

We want to generate artificial corpora with some of the statistical features of a given corpus. This function generates one document with *L* words (number obtained from the fitted lognormal distribution) out of a vocabulary with *numTerms* terms (number extracted from the original corpus).

```{r eval=FALSE, warning=FALSE}
createDocument<-function(L, numTerms,r0){
  counts<-rep(0,numTerms)
  r<-r0
  text<-rep(0,L)
  df<-cbind(r,counts)
  
  p<-1/seq(1,numTerms,1)
  #Normalization constant
  C<-1/sum(p)
  #probability density function
  pd<-C*p
  #ecdf
  ecdf=cumsum(pd)
  probs <- replicate(L,length(ecdf[ecdf<runif(1)])+1)
  i=1
  ord<-rep(0,numTerms)
  while(i<=L){
    #It orders first according to the counts and then to the original ranking r0
    ind=probs[i]
    ######One line solution
    #df<-df[order(-df[,2],match(df[,1],r0)),]
    #text[i]<-df[,1][probs[i]]
    #df[,2][probs[i]]<-df[,2][probs[i]]+1
    
    ######Solution without modifying df
    #Since df is constant the column of rankings will always corresponds to the original
    #one, thus no extra vector is needed
    ord<-order(-df[,2],df[,1])
    text[i]<-df[ord[ind],1]
    df[ord[ind],2]<-df[ord[ind],2]
    
    #####Solution using a manual ordering coded in C.
    #text[i]<-df[ind,1]
    #df[ind,2]<-df[ind,2]+1
    #df<-reorder(df,r0,ind)
    #df<-cbind(df[1:numTerms],df[(numTerms+1):(2*numTerms)])
    
    #####Manual solution in R.
    #j=ind-1
    #temp=df[ind,]
    #repeat{
    #  if(j==0 || df[ind,2]<df[j,2] || (df[ind,2]==df[j,2] && match(df[ind,1],r0)>match(df[j,1],r0))){
    #    if(j==ind-1){
    #      break
    #    }
    #    else{
    #      df[(j+2):ind,]<-df[(j+1):(ind-1),]
    #      df[j+1,]<-temp
    #      break
    #    }
    #    
    #  }
    #  else{
    #    j<-j-1
    #  }
    #}
    i<-i+1
  }
  
  #One document with size L is generated and returned
  text<-paste(text,collapse=" ")
  cc<-Corpus(VectorSource(text))
 
  return(cc)
  
}
```

The actual creation of the artifial corpus is carried out in the next three lines. A vector of document sizes is obtained using the fitted lognormal and the former function is applied over it with the original rankins as another parameter (for the algorithm with memory this line has to be modified). Finally the corpus is created and saved in a text file.

```{r eval=FALSE, warning=FALSE}
#Vector with the document sizes
Ls<-ceiling(rlnorm(numDocs,meanlog = meanlogemp, sdlog = sdlogemp))
#Initial ranking
r0<-seq(1,numTerms,1)
#Creating the artificial corpus
cc<-lapply(Ls,createDocument,numTerms=numTerms,r0=r0)
docs <- Corpus(VectorSource(unlist(cc)))
#Saving it into a file
writeLines(as.character(docs), con="artificialCorpus1.txt")
```

The corpus can be retrieved from memory with:

```{r, warning=FALSE}
#Reading it from memory
conn<-file("artificialCorpus1.txt",open="r")
d<-readLines(conn)
close(conn)
docs1<-Corpus(VectorSource(d))
Tdm1 <- TermDocumentMatrix(docs1, control=list(wordLengths=c(1,Inf)))
```


## Analysing the corpora

The comparison between original and simulated corpora is  done via the following functions:

Function to list the cumulative vocabulary size as we add more documents.

```{r, warning=FALSE}
cum_vocabulary_size<-function (m) 
{
  ###This line gives the number of the document in which each word appears for the first time.
  i <- sapply(split(m$i, m$j), min)
  ###This creates a table of the number of new words in each document, counting the number of times 
  # each document appears in the table above. 
  tab <- table(i)
  
  v <- double(nrow(m))
  v[as.numeric(names(tab))] <- tab
  #It creates the cumulative sums of words in the documents.
  cumsum(v)
}
```

Function to plot Zipf's law for the two corpora:

```{r, warning=FALSE}
comparisonZipf<-function(Tdm,Tdm1){
  
  x<-t(Tdm)
  x1<-t(Tdm1)
  y <- log(sort(slam::col_sums(x), decreasing = TRUE))
  x <- log(seq_along(y))
  y1 <- log(sort(slam::col_sums(x1), decreasing = TRUE))
  x1 <- log(seq_along(y1))
  plot(x,y,col="blue",xlab="log(rank)",ylab="log(frequency)",main="Zipf's law")
  points(x1,y1,col="red")
  legend("topright", c("Original IS corpus","Simulated corpus"),lty=c(1,1),col=c("blue","red"))
  
}
```

Function to plot Heap's law for the two corpora:

```{r, warning=FALSE}
comparisonHeap<-function(Tdm,Tdm1){
  
  x<-t(Tdm)
  x1<-t(Tdm1)
  y <- log(cum_vocabulary_size(x))
  x <- log(cumsum(slam::row_sums(x)))
  y1 <- log(cum_vocabulary_size(x1))
  x1 <- log(cumsum(slam::row_sums(x1)))
  plot(x,y,col="blue",xlab="log(T)",ylab="log(V)",main="Heap's law")
  points(x1,y1,col="red")
  legend("topleft", c("Original IS corpus","Simulated corpus"),lty=c(1,1),col=c("blue","red"))
  
}
```

Generating the plots:

```{r, warning=FALSE}
comparisonZipf(Tdm,Tdm1)
comparisonHeap(Tdm,Tdm1)
```



## Other corpora

Distribution of document size Child1:

```{r echo=FALSE, warning=FALSE}
conn<-file("stemCorpusChild1.txt",open="r")
d<-readLines(conn)
close(conn)
docs<-Corpus(VectorSource(d))

Tdm1 <- TermDocumentMatrix(docs, control=list(wordLengths=c(1,Inf)))
x<-t(Tdm1)
#Number of documents
numDocs<-nrow(x)
#Number of terms
numTerms<-ncol(x)
#Number of words in each document
nOW<-as.vector(slam::row_sums(x))

mle<-fitdistr(nOW[nOW>0], "lognormal")
meanlogemp <- mle$estimate["meanlog"]
sdlogemp <- mle$estimate["sdlog"]

ks.test(nOW[nOW>0], plnorm, meanlog = meanlogemp, sdlog = sdlogemp) 


maxDoc<-max(nOW)
mydata_hist<-hist(nOW,probability = TRUE, breaks = maxDoc, col = "darkslategray4", border = "seashell3")

z<-seq(0,1500,1)

#Calculates the probability that a document has certain number of words, using data from the histogram produced above.
prob<-mydata_hist$count/numDocs
#Plot empirical vs fitted distribution density functions
plot(mydata_hist$mid,log="xy",prob,lwd=1, lend=1)
lines(dlnorm(z,meanlog = meanlogemp, sdlog = sdlogemp),   # Add the kernel density estimate (-.5 fix for the bins)
      col = "firebrick2", lwd = 3)
```


Distribution of document size Child2:

```{r echo=FALSE, warning=FALSE}
conn<-file("stemCorpusChild2.txt",open="r")
d<-readLines(conn)
close(conn)
docs<-Corpus(VectorSource(d))

Tdm2 <- TermDocumentMatrix(docs, control=list(wordLengths=c(1,Inf)))
x<-t(Tdm2)
#Number of documents
numDocs<-nrow(x)
#Number of terms
numTerms<-ncol(x)
#Number of words in each document
nOW<-as.vector(slam::row_sums(x))

mle<-fitdistr(nOW[nOW>0], "lognormal")
meanlogemp <- mle$estimate["meanlog"]
sdlogemp <- mle$estimate["sdlog"]

ks.test(nOW[nOW>0], plnorm, meanlog = meanlogemp, sdlog = sdlogemp) 


maxDoc<-max(nOW)
mydata_hist<-hist(nOW,probability = TRUE, breaks = maxDoc, col = "darkslategray4", border = "seashell3")

z<-seq(0,1500,1)

#Calculates the probability that a document has certain number of words, using data from the histogram produced above.
prob<-mydata_hist$count/numDocs
#Plot empirical vs fitted distribution density functions
plot(mydata_hist$mid,log="xy",prob,lwd=1, lend=1)
lines(dlnorm(z,meanlog = meanlogemp, sdlog = sdlogemp),   # Add the kernel density estimate (-.5 fix for the bins)
      col = "firebrick2", lwd = 3)
```

Distribution of document size Child3:

```{r echo=FALSE, warning=FALSE}




conn<-file("stemCorpusChild3.txt",open="r")
d<-readLines(conn)
close(conn)
docs<-Corpus(VectorSource(d))

Tdm3 <- TermDocumentMatrix(docs, control=list(wordLengths=c(1,Inf)))
x<-t(Tdm3)
#Number of documents
numDocs<-nrow(x)
#Number of terms
numTerms<-ncol(x)
#Number of words in each document
nOW<-as.vector(slam::row_sums(x))

mle<-fitdistr(nOW[nOW>0], "lognormal")
meanlogemp <- mle$estimate["meanlog"]
sdlogemp <- mle$estimate["sdlog"]

ks.test(nOW[nOW>0], plnorm, meanlog = meanlogemp, sdlog = sdlogemp) 


maxDoc<-max(nOW)
mydata_hist<-hist(nOW,probability = TRUE, breaks = maxDoc, col = "darkslategray4", border = "seashell3")

z<-seq(0,1500,1)

#Calculates the probability that a document has certain number of words, using data from the histogram produced above.
prob<-mydata_hist$count/numDocs
#Plot empirical vs fitted distribution density functions
plot(mydata_hist$mid,log="xy",prob,lwd=1, lend=1)
lines(dlnorm(z,meanlog = meanlogemp, sdlog = sdlogemp),   # Add the kernel density estimate (-.5 fix for the bins)
      col = "firebrick2", lwd = 3)
```


Distribution of document size Child4:

```{r echo=FALSE, warning=FALSE}

conn<-file("stemCorpusChild4.txt",open="r")
d<-readLines(conn)
close(conn)
docs<-Corpus(VectorSource(d))

Tdm4 <- TermDocumentMatrix(docs, control=list(wordLengths=c(1,Inf)))

x<-t(Tdm4)
#Number of documents
numDocs<-nrow(x)
#Number of terms
numTerms<-ncol(x)
#Number of words in each document
nOW<-as.vector(slam::row_sums(x))

mle<-fitdistr(nOW[nOW>0], "lognormal")
meanlogemp <- mle$estimate["meanlog"]
sdlogemp <- mle$estimate["sdlog"]

ks.test(nOW[nOW>0], plnorm, meanlog = meanlogemp, sdlog = sdlogemp) 


maxDoc<-max(nOW)
mydata_hist<-hist(nOW,probability = TRUE, breaks = maxDoc, col = "darkslategray4", border = "seashell3")
````


```{r echo=FALSE, warning=FALSE}
z<-seq(0,1500,1)

#Calculates the probability that a document has certain number of words, using data from the histogram produced above.
prob<-mydata_hist$count/numDocs
#Plot empirical vs fitted distribution density functions
plot(mydata_hist$mid,log="xy",prob,lwd=1, lend=1)
lines(dlnorm(z,meanlog = meanlogemp, sdlog = sdlogemp),   # Add the kernel density estimate (-.5 fix for the bins)
      col = "firebrick2", lwd = 3)
```

Distribution of the Santa Barbara corpus.

```{r echo=FALSE, warning=FALSE,eval=FALSE}
conn<-file("sbc.fixed.txt",open="r")
d<-readLines(conn)
close(conn)
singleString <- paste(d,collapse="\n")
Z <- str_split(singleString,pattern="\\\n(\\s)?\\\n")[[1]]
docs<-Corpus(VectorSource(Z))

#docs <- tm_map(docs, function(x) stri_replace_all_regex(x, "[[:punct:]]", " "))
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, c(stopwords("english"),"th","s"))
docs <- tm_map(docs, stripWhitespace)
docs1<-docs
docs <- tm_map(docs, stemDocument,language="english")
docs <- tm_map(docs, PlainTextDocument)
#for(i in 1:length(docs)){
#  docs[[i]]$meta<-NULL
#}
#docs$meta<-NULL
writeLines(as.character(docs), con="stemSBC.txt")
writeLines(as.character(docs1), con="SBC.txt")
````

```{r echo=FALSE, warning=FALSE}
conn<-file("stemSBC.txt",open="r")
d<-readLines(conn)
close(conn)
docs<-Corpus(VectorSource(d))

Tdm5 <- TermDocumentMatrix(docs, control=list(wordLengths=c(1,Inf)))

x<-t(Tdm5)
#Number of documents
numDocs<-nrow(x)
#Number of terms
numTerms<-ncol(x)
#Number of words in each document
nOW<-as.vector(slam::row_sums(x))

mle<-fitdistr(nOW[nOW>0], "lognormal")
meanlogemp <- mle$estimate["meanlog"]
sdlogemp <- mle$estimate["sdlog"]

ks.test(nOW[nOW>0], plnorm, meanlog = meanlogemp, sdlog = sdlogemp) 


maxDoc<-max(nOW)
mydata_hist<-hist(nOW,probability = TRUE, breaks = maxDoc, col = "darkslategray4", border = "seashell3")

z<-seq(0,1500,1)

#Calculates the probability that a document has certain number of words, using data from the histogram produced above.
prob<-mydata_hist$count/numDocs
#Plot empirical vs fitted distribution density functions
plot(mydata_hist$mid,log="xy",prob,lwd=1, lend=1)
lines(dlnorm(z,meanlog = meanlogemp, sdlog = sdlogemp),   # Add the kernel density estimate (-.5 fix for the bins)
      col = "firebrick2", lwd = 3)
```

Distribution of the TASA corpus.

```{r echo=FALSE, warning=FALSE, eval=FALSE}
conn<-file("tasa.corpus.txt",open="r")
d<-readLines(conn)
close(conn)
singleString <- paste(d,collapse="\n")
Z <- str_split(singleString,pattern="\\\n(\\s)?\\\n")[[1]]
docs<-Corpus(VectorSource(Z))

#docs <- tm_map(docs, function(x) stri_replace_all_regex(x, "[[:punct:]]", " "))
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, c(stopwords("english"),"th","s"))
docs <- tm_map(docs, stripWhitespace)
docs1<-docs
docs <- tm_map(docs, stemDocument,language="english")
docs <- tm_map(docs, PlainTextDocument)
#for(i in 1:length(docs)){
#  docs[[i]]$meta<-NULL
#}
#docs$meta<-NULL
writeLines(as.character(docs), con="stemTasa.txt")
writeLines(as.character(docs1), con="Tasa.txt")
```

```{r echo=FALSE, warning=FALSE}
conn<-file("stemTasa.txt",open="r")
d<-readLines(conn)
close(conn)
docs<-Corpus(VectorSource(d))

Tdm6 <- TermDocumentMatrix(docs, control=list(wordLengths=c(1,Inf)))

x<-t(Tdm6)
#Number of documents
numDocs<-nrow(x)
#Number of terms
numTerms<-ncol(x)
#Number of words in each document
nOW<-as.vector(slam::row_sums(x))

mle<-fitdistr(nOW[nOW>0], "lognormal")
meanlogemp <- mle$estimate["meanlog"]
sdlogemp <- mle$estimate["sdlog"]

ks.test(nOW[nOW>0], plnorm, meanlog = meanlogemp, sdlog = sdlogemp) 


maxDoc<-max(nOW)
mydata_hist<-hist(nOW,probability = TRUE, breaks = maxDoc, col = "darkslategray4", border = "seashell3")

z<-seq(0,1500,1)

#Calculates the probability that a document has certain number of words, using data from the histogram produced above.
prob<-mydata_hist$count/numDocs
#Plot empirical vs fitted distribution density functions
#plot(mydata_hist$mid,log="xy",prob,lwd=1, lend=1)
#lines(dlnorm(z,meanlog = meanlogemp, sdlog = sdlogemp),   # Add the kernel density estimate (-.5 fix for the bins)
#      col = "firebrick2", lwd =3)
````

# Text analysis.

Original vs Simulated Corpora Child2:

```{r echo=FALSE, warning=FALSE}
#Reading it from memory
conn<-file("artificialCorpus1Child2.txt",open="r")
d<-readLines(conn)
close(conn)
docs1<-Corpus(VectorSource(d))
Tdm21 <- TermDocumentMatrix(docs1, control=list(wordLengths=c(1,Inf)))
```

```{r, warning=FALSE}
comparisonZipf(Tdm2,Tdm21)
comparisonHeap(Tdm2,Tdm21)
```

Comparison of all original corpora:

```{r echo=FALSE, warning=FALSE}
comparisonZipfAll<-function(Tdm,Tdm1,Tdm2,Tdm3,Tdm4,Tdm5,Tdm6){
  
  x<-t(Tdm)
  x1<-t(Tdm1)
  x2<-t(Tdm2)
  x3<-t(Tdm3)
  x4<-t(Tdm4)
  x5<-t(Tdm5)
  x6<-t(Tdm6)
  
  y <- log(sort(slam::col_sums(x), decreasing = TRUE))
  x <- log(seq_along(y))
  y1 <- log(sort(slam::col_sums(x1), decreasing = TRUE))
  x1 <- log(seq_along(y1))
  y2 <- log(sort(slam::col_sums(x2), decreasing = TRUE))
  x2 <- log(seq_along(y2))
  y3 <- log(sort(slam::col_sums(x3), decreasing = TRUE))
  x3 <- log(seq_along(y3))
  y4 <- log(sort(slam::col_sums(x4), decreasing = TRUE))
  x4 <- log(seq_along(y4))
  y5 <- log(sort(slam::col_sums(x5), decreasing = TRUE))
  x5 <- log(seq_along(y5))
  y6 <- log(sort(slam::col_sums(x6), decreasing = TRUE))
  x6 <- log(seq_along(y6))
  
  plot(x,y,col="blue",xlab="log(rank)",ylab="log(frequency)",main="Zipf's law")
  points(x1,y1,col="red")
  points(x2,y2,col="green")
  points(x3,y3,col="yellow")
  points(x4,y4,col="black")
  points(x5,y5,col="brown1")
  points(x6,y6,col="coral1",pch=1)
  legend("topright", c("IS corpus","Child1","Child2","Child3","Child4","SBC","TASA"),
         lty=c(1,1),col=c("blue","red","green","yellow","black","brown1","coral1"))
  
}

comparisonHeapAll<-function(Tdm,Tdm1,Tdm2,Tdm3,Tdm4,Tdm5,Tdm6){
  
  x<-t(Tdm)
  x1<-t(Tdm1)
  x2<-t(Tdm2)
  x3<-t(Tdm3)
  x4<-t(Tdm4)
  x5<-t(Tdm5)
  x6<-t(Tdm6)
  y <- log(cum_vocabulary_size(x))
  x <- log(cumsum(slam::row_sums(x)))
  y1 <- log(cum_vocabulary_size(x1))
  x1 <- log(cumsum(slam::row_sums(x1)))
  y2 <- log(cum_vocabulary_size(x2))
  x2 <- log(cumsum(slam::row_sums(x2)))
  y3 <- log(cum_vocabulary_size(x3))
  x3 <- log(cumsum(slam::row_sums(x3)))
  y4 <- log(cum_vocabulary_size(x4))
  x4 <- log(cumsum(slam::row_sums(x4)))
  y5 <- log(cum_vocabulary_size(x5))
  x5 <- log(cumsum(slam::row_sums(x5)))
  y6 <- log(cum_vocabulary_size(x6))
  x6 <- log(cumsum(slam::row_sums(x6)))
  plot(x,y,col="blue",xlab="log(T)",ylab="log(V)",main="Heap's law")
  points(x1,y1,col="red")
  points(x2,y2,col="green")
  points(x3,y3,col="yellow")
  points(x4,y4,col="black")
  points(x5,y5,col="brown1")
  points(x6,y6,col="coral1",pch=1)
  
  legend("topleft", c("IS corpus","Child1","Child2","Child3","Child4","SBC","TASA"),
         lty=c(1,1),col=c("blue","red","green","yellow","black","brown1","coral1"))
}

comparisonZipfAll(Tdm,Tdm1,Tdm2,Tdm3,Tdm4,Tdm5,Tdm6)
comparisonHeapAll(Tdm,Tdm1,Tdm2,Tdm3,Tdm4,Tdm5,Tdm6)
```

